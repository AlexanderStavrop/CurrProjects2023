# import modules 
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd 
import random

k = 10    #number of arms
T = 1000  #horizon  
N = 20    #explore rounds


bandit = np.random.random((k,)) #success prob. for each arm
best = np.amax(bandit) #best arm


bandit_score = np.zeros((k,)) #total score of each arm for first N rounds
pulls = np.zeros((k,)) #num of arm pulls
inst_score = np.zeros((T,)) #reward for round t
best_score = np.zeros((T,)) #cumulative reward of best arm for round t
alg_score = np.zeros((T,)) #cumulative reward for round t
regret =  np.zeros((T,)) #regret for round t


# This code properly activates each arm in round robin, for a total of N times

for i in range(N):
  for j in range(k):
    score = np.random.binomial(1,p=bandit[j])   # get a reward for arm j
    inst_score[i*k+j] = score                   # record reward of algorithm (during explore) at that instant
    bandit_score[j] += score                    # update the total score (during explore) of arm j


for i in range(k):
  print('arm = %d: true mean = %f : sample mean = %f' % (i,bandit[i],bandit_score[i]/N))

arm = np.argmax(bandit_score/N)  #get id of best arm (during explore)


for i in range(N*k,T):
  inst_score[i] = np.random.binomial(1,p=bandit[arm])   # Play best arm for the remainder of the horizon

for i in range(T):
  if i > 0:
    best_score[i] = best_score[i-1] + best              # Vector keeping track of t*optimal reward (cummulative reward)
  else: 
    best_score[i] = best

  if i > 0: 
    alg_score[i] = alg_score[i-1] + inst_score[i] #vector keeping track of cummulative explore-then-eploit reward at all times 
  else: 
    alg_score[i] = inst_score[i]

  regret[i] = (best_score[i] - alg_score[i])/(i+1)  #regret per iteration at round t